{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing all the essential libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Importing basic NLTK library ###\n",
    "import nltk\n",
    "### Importing Tokenization algorithms from NLTK ###\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "### Importing the DefaultDict library from Python\n",
    "from collections import defaultdict\n",
    "### Importing Built-in-Python function for n-largest elements in a list ###\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Class FreqSummarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The FreqSummarizer class will:\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;1. Eliminate the stopwords <br/>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;2. Find the frequency of the words <br/>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;3. Assign a score of importance for each word <br/>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;4. Rank the sentences based on the score <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FreqSummarizer:\n",
    "    \n",
    "### Initializing the characteristics of the Member function\n",
    "    def __init__(self, min_cut = 0.1, max_cut = 0.9):\n",
    "        self.min_cut = min_cut\n",
    "        self.max_cut = max_cut\n",
    "        self._stopwords = set(stopwords.words('english') + list(punctuation))\n",
    "    \n",
    "### Function for the Dictionary of Words:Frequencies as the key-value pair\n",
    "    def _compute_freq(self, word_sent):\n",
    "        freq = defaultdict(int)\n",
    "        \n",
    "###     Iterating through the words in sentences for incrementing the Frequency count\n",
    "        for sentence in word_sent:\n",
    "            for word in sentence:\n",
    "                if word not in self._stopwords:\n",
    "                    freq[word] += 1\n",
    "\n",
    "###     Normalizing the Frequency and Pruning the results based on Caps\n",
    "        max_freq = float(max(freq.values()))\n",
    "        for word in list(freq.keys()):\n",
    "            freq[word] = freq[word]/max_freq\n",
    "            if freq[word] >= self.max_cut or freq[word] <= self.min_cut:\n",
    "                del freq[word]\n",
    "        return freq\n",
    "    \n",
    "### Function for assigning a score to a sentence based on the Frequency of words\n",
    "    def summarizer(self, text, n):\n",
    "###     Splitting text into list of sentences\n",
    "        sents = sent_tokenize(text)\n",
    "###     Performing a sanity check on the inputted article\n",
    "        assert n <= len(sents)\n",
    "###     Creating a list of words from the sentences in the article\n",
    "        word_sent = [word_tokenize(s.lower()) for s in sents]\n",
    "    \n",
    "        self._freq = self._compute_freq(word_sent)\n",
    "        \n",
    "        ranking = defaultdict(int)\n",
    "\n",
    "###     Creating a tuple of indices and sentences and incrementing the Rankings dictionary\n",
    "        for i, sent in enumerate(word_sent):\n",
    "            for word in sent:\n",
    "                if word in self._freq:\n",
    "                    ranking[i] += self._freq[word]\n",
    "\n",
    "###     Sorting the sentences based on their frequencies\n",
    "        sents_idx = nlargest(n, ranking, key = ranking.get)\n",
    "        \n",
    "        return [sents[j] for j in sents_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the Article and Extracting the Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the essential libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining a Function to Extract the Text from the Article URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_text_wp(url):\n",
    "### Downloading the Page's HTML content    \n",
    "    page = urllib.request.urlopen(url).read().decode('utf8')\n",
    "### Initializing a Soup object    \n",
    "    soup = BeautifulSoup(page)\n",
    "### Extracting and combining all the <p> content from the <article> tag\n",
    "    text = ' '.join(map(lambda p: p.text, soup.find_all('article')))\n",
    "### Initializing a second Soup object\n",
    "    soup2 = BeautifulSoup(text)\n",
    "### Joining \n",
    "    text = ' '.join(map(lambda p: p.text, soup2.find_all('p')))\n",
    "    \n",
    "    return soup.title.text, text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto-Summarization of the Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "justAnotherUrl = \"https://www.washingtonpost.com/news/innovations/wp/2014/10/01/the-incredible-potential-and-dangers-of-data-mining-health-records/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chiku\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "justTheText = extract_text_wp(justAnotherUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fs = FreqSummarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary = fs.summarizer(justTheText[1], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TITLE: The incredible potential and dangers of data mining health records - The Washington Post\u001b[0m\n",
      "\n",
      "“You really have to battle with Silicon Valley and the Boston academic scene.” “Why would someone who is really really good at analyzing data come to work for a health care organization and make X dollars when they could go to Google and make 10X dollars?” Marko added.\n",
      "\n",
      "While they universally agree that data mining — the examination and analysis of huge batches of information — could invigorate health care, they caution that any sort of accurate estimate would be impossible.\n",
      "\n",
      "It’s the kind of potential Google chief executive Larry Page hinted at when he told the New York Times earlier this year that “we’d probably save 100,000 lives next year,” if we data mined health care data.\n"
     ]
    }
   ],
   "source": [
    "print('TITLE:' + (justTheText[0]) + '\\033[0m' + \"\\n\\n\" + \"\\n\\n\".join(summary))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
